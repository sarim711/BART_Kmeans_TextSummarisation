{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarim711/BART_Kmeans_TextSummarisation/blob/main/Text_Summarisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePLrbmyyYx2K"
      },
      "source": [
        "# Generating medical text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUJRND628Rl6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvc3CVYu8d4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "62c0bd38-464e-465d-def2-e816e12483df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = pd.read_csv(\"mg_small.csv\",usecols = [\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 10)  # Adjust as needed to avoid truncation\n",
        "df.head(2)"
      ],
      "metadata": {
        "id": "7IqHCpveBw1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(text):\n",
        "\n",
        "     '''removes anything other than standard punctuation\n",
        "        and letters. Sentences should be separated by a period and a single space.\n",
        "        using re for this.'''\n",
        "    #Remove non-standard characters\n",
        "    # text = re.sub(r'[^a-zA-Z\\.\\s]', '', text)\n",
        "\n",
        "    #After updating the code to include numbers\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\.\\s]', '', text)\n",
        "\n",
        "    #Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    #Ensure proper sentence separation (period followed by a single space)\n",
        "    text = re.sub(r'\\.(?=[^\\s])', '. ', text)\n",
        "\n",
        "    #Strip leading/trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "print(clean(df.text.iloc[0]))\n",
        "df.cleaned = df.text.apply(clean)"
      ],
      "metadata": {
        "id": "pb7FDF5yB1N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def calculate_word_frequency(document):\n",
        "    '''return a dictionary that contains the\n",
        "       frequencies of each word in the document'''\n",
        "    #tokenize document into words\n",
        "    words = re.findall(r'\\b\\w+\\b', document.lower())\n",
        "\n",
        "    # Calculate the frequency of each word\n",
        "    word_count = Counter(words)\n",
        "\n",
        "    # Normalize the frequency by the total number of words\n",
        "    total_words = sum(word_count.values())\n",
        "    word_freq = {word: count / total_words for word, count in word_count.items()}\n",
        "\n",
        "    return word_freq\n",
        "\n",
        "word_freq = calculate_word_frequency(df.cleaned.iloc[0])\n",
        "word_freq"
      ],
      "metadata": {
        "id": "OoNaxqIFB4ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(document, word_freq):\n",
        "    '''Calculating the representativeness score of each sentence\n",
        "       by summing up the frequency of the words in each sentence,\n",
        "       then dividing by sentence length'''\n",
        "    # Split the document into sentences\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w\\.)(?<![A-Z][a-z]\\.)(?<=\\.\\s|\\?\\s|\\!\\s)', document)\n",
        "\n",
        "    # Calculate scores for each sentence\n",
        "    scores = []\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
        "        sentence_score = sum(word_freq.get(word, 0) for word in words)\n",
        "        if len(words) >5 :\n",
        "            sentence_score /= len(words)  # Normalize by sentence length\n",
        "        scores.append(sentence_score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "sentence_scores = calculate_sentence_scores(df.cleaned.iloc[0], word_freq)\n",
        "sentence_scores[:3]"
      ],
      "metadata": {
        "id": "BtJhYBttB8Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_sentences(document, sentence_scores, k):\n",
        "  '''Returning a list containing the top k most representative sentences'''\n",
        "    # Split the document into sentences\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w\\.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', document)\n",
        "\n",
        "    # Pair each sentence with its score and sort by score (descending order)\n",
        "    ranked_sentences = sorted(zip(sentences, sentence_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select the top-k sentences\n",
        "    top_k_sentences = [sentence for sentence, score in ranked_sentences[:k]]\n",
        "\n",
        "    return top_k_sentences\n",
        "top_k_sentences = get_top_k_sentences(df.cleaned.iloc[0], sentence_scores,3)\n",
        "top_k_sentences"
      ],
      "metadata": {
        "id": "6SAHS1TQB997"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_summary(document, top_k):\n",
        "    cleaned = clean(document)\n",
        "    word_freq = calculate_word_frequency(cleaned)\n",
        "    sentence_scores = calculate_sentence_scores(cleaned, word_freq)\n",
        "    top_k_sentences = get_top_k_sentences(document, sentence_scores, top_k)\n",
        "    summary = \". \".join(top_k_sentences)\n",
        "    return summary\n",
        "\n",
        "df[\"statistical_summary\"] = df.text.progress_apply(lambda text: create_summary(text, 3))"
      ],
      "metadata": {
        "id": "8HZBXWQqCDjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust pandas settings to display full lines\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 10)  # Adjust as needed to avoid truncation\n",
        "\n",
        "df['statistical_summary']"
      ],
      "metadata": {
        "id": "OibWpXTkCHtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFpkV00P_vt1"
      },
      "source": [
        "## K-means\n",
        "\n",
        "We now graduate to using neural networks to improve our summarizations. The way this is done is by the following procedure:\n",
        "\n",
        "1) Embed each sentence using bio-medical BERT (use whichever model you prefer, I enjoy PubMedBERT). Unfortunately, we cannot use the CLS token to create a general sentence embedding because that requires task-specific downstream training for a labeled dataset. Therefore, we take the average over the tokens in the sentence to create an embedding.\n",
        "\n",
        "2) Cluster the sentence embeddings using [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).\n",
        "\n",
        "3) For each cluster, choose the sentence that is closest to that cluster's centroid. Order the sentences according to their in-text appearance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM,AutoModel\n",
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the PubMedBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\").to(device)\n",
        "\n",
        "def embed_using_bert(bert_model, tokenizer, document):\n",
        "    sentences = document.split(\". \")\n",
        "    sentence_embeddings = []\n",
        "\n",
        "    for sentence_index, sentence in enumerate(sentences):\n",
        "        # Tokenize the sentence and move input tensors to GPU if available\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # Extract embeddings for tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**tokens)\n",
        "\n",
        "        # Get the token embeddings (hidden states of the last layer)\n",
        "        token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "        # Compute the mean embedding for the sentence\n",
        "        sentence_embedding = token_embeddings.mean(dim=0).cpu().numpy()  # Move back to CPU\n",
        "        sentence_embeddings.append(sentence_embedding)\n",
        "\n",
        "    return sentences, np.array(sentence_embeddings)\n"
      ],
      "metadata": {
        "id": "fKKp725XCsWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_sentence_embeddings(sentence_embeddings, sentences, k_clusters):\n",
        "    # Remove duplicate embeddings\n",
        "    unique_embeddings, unique_indices = np.unique(sentence_embeddings, axis=0, return_index=True)\n",
        "    unique_sentences = [sentences[i] for i in unique_indices]\n",
        "\n",
        "    # Adjust k to the number of unique embeddings\n",
        "    k_clusters = min(k_clusters, len(unique_embeddings))\n",
        "\n",
        "    if k_clusters == 0:  # No sentences to cluster\n",
        "        return \"\"\n",
        "\n",
        "    # Perform K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=k_clusters, random_state=42).fit(unique_embeddings)\n",
        "    cluster_attributions = kmeans.labels_\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # Compute the sentence closest to each cluster center\n",
        "    representative_sentences = []\n",
        "    for cluster_idx in range(k_clusters):\n",
        "        cluster_indices = np.where(cluster_attributions == cluster_idx)[0]\n",
        "        if len(cluster_indices) == 0:\n",
        "            continue  # Skip empty clusters\n",
        "        cluster_embeddings = unique_embeddings[cluster_indices]\n",
        "        cluster_centroid = cluster_centers[cluster_idx]\n",
        "\n",
        "        # Find the sentence closest to the centroid\n",
        "        closest_idx = cluster_indices[np.argmin(np.linalg.norm(cluster_embeddings - cluster_centroid, axis=1))]\n",
        "        representative_sentences.append((unique_indices[closest_idx], unique_sentences[closest_idx]))\n",
        "\n",
        "    # Order sentences by their appearance in the original document\n",
        "    representative_sentences.sort(key=lambda x: x[0])\n",
        "    k_sentences_in_order = [sentence for _, sentence in representative_sentences]\n",
        "\n",
        "    return \". \".join(k_sentences_in_order)\n"
      ],
      "metadata": {
        "id": "gdOnwAD0CwWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_document_with_kmeans(document, k):\n",
        "    #Embed sentences\n",
        "    sentences, embeddings = embed_using_bert(model, tokenizer, document)\n",
        "\n",
        "    if len(sentences) >= k:\n",
        "        summary = cluster_sentence_embeddings(embeddings, sentences, k)\n",
        "    else:\n",
        "\n",
        "        summary = document if len(sentences) < 2 else \". \".join(sentences[:k])\n",
        "\n",
        "    return summary\n",
        "\n",
        "tqdm.pandas()  # Enable progress bar for DataFrame operations\n",
        "df['cleaned'] = df['text'].apply(clean)\n",
        "df['neural_summary'] = df['cleaned'].progress_apply(lambda text: summarize_document_with_kmeans(text, k=5))"
      ],
      "metadata": {
        "id": "BLEkgfrTC1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 10)  # Adjust as needed to avoid truncation\n",
        "df['neural_summary']"
      ],
      "metadata": {
        "id": "tKmWhH4PC4bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "   Reduce dataset to 10,000 cleaned text as we have limited GPU access time on collab\n",
        "   From Reduced cleaned text docs we can still analyze summary quality effectively, while varying k.\n",
        "'''\n",
        "# Reduce the dataset to 10,000 documents for faster processing\n",
        "df = df.sample(n=10000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Reapply the cleaned column processing if not already done\n",
        "if 'cleaned' not in df.columns:\n",
        "    df['cleaned'] = df['text'].apply(clean)\n",
        "\n"
      ],
      "metadata": {
        "id": "cDBiYR9kC8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing k to 3\n",
        "\n",
        "df['neural_summary'] = df['cleaned'].progress_apply(lambda text: summarize_document_with_kmeans(text, k=3))\n",
        "df['neural_summary']"
      ],
      "metadata": {
        "id": "sNcgnN-zC_Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing k to 7\n",
        "df['neural_summary'] = df['cleaned'].progress_apply(lambda text: summarize_document_with_kmeans(text, k=7))\n",
        "df['neural_summary']"
      ],
      "metadata": {
        "id": "sdf2HeaxDB6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing token embeddings to max\n",
        "def embed_using_bert(bert_model, tokenizer, document):\n",
        "    sentences = document.split(\". \")\n",
        "    sentence_embeddings = []\n",
        "\n",
        "    for sentence_index, sentence in enumerate(sentences):\n",
        "        # Tokenize the sentence and move input tensors to GPU if available\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "        # Extract embeddings for tokens\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**tokens)\n",
        "\n",
        "        # Get the token embeddings (hidden states of the last layer)\n",
        "        token_embeddings = outputs.last_hidden_state.squeeze(0)\n",
        "\n",
        "        # Compute the embedding for the sentence\n",
        "        sentence_embedding = token_embeddings.max(dim=0).values.cpu().numpy()  # Move to CPU before converting to NumPy\n",
        "        sentence_embeddings.append(sentence_embedding)\n",
        "\n",
        "    return sentences, np.array(sentence_embeddings)\n",
        "\n",
        "df['neural_summary'] = df['cleaned'].progress_apply(lambda text: summarize_document_with_kmeans(text, k=5))\n",
        "\n",
        "df['neural_summary']"
      ],
      "metadata": {
        "id": "8NlBEL7QDEYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1qhIi-WGwMz"
      },
      "source": [
        "## BART\n",
        "\n",
        "We will now use dedicated sequence-to-sequence neural networks. These models learn to map input sequences of text to different length output sequences of text. They do this with larged labeled summary datasets. It would be great to train (or even fine-tune) our own bio-medical summarization model, but unfortunately this requires datasets which do not exist (to my knowledge). Thus, we will use [BART](https://huggingface.co/facebook/bart-large), a general purpose encoder-decoder model from meta. This is the most opaque but potentially best performing summarization model, as it deals with actual text generation. Unlike the previous models, which pick and choose pre-existing texts, sequence-to-sequence models generate a completely new summary. Thus, there is only one step: model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU available!')\n",
        "    device = torch.cuda.current_device()\n",
        "else:\n",
        "    print('GPU unavailable - CPU will be used for all calculations')\n",
        "    device = None\n",
        "\n",
        "# Tokenizer and model loading for bart-large-cnn\n",
        "\n",
        "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
        "\n",
        "\n",
        "#for multi-line output, thanks to https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "YLkBE-SmDJZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transmitting the encoded inputs to the model.generate() function\n",
        "inputs = tokenizer.batch_encode_plus([df.cleaned.iloc[10]],return_tensors='pt').to(device)\n",
        "summary_ids =  model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
        "# Decoding and printing the summary\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(summary)\n",
        "print(\"\\n===\\n\")\n",
        "print(df.cleaned.iloc[10])"
      ],
      "metadata": {
        "id": "p43yyB66DNsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''reduced dataset to 2000 cleaned texts as the entire dataset was expected to take 5 hours, GPU would'nt ave lasted that long'''\n",
        "df = df.sample(n=2000, random_state=42).reset_index(drop=True)\n",
        "# Reapply the cleaned column processing if not already done\n",
        "if 'cleaned' not in df.columns:\n",
        "    df['cleaned'] = df['text'].apply(clean)\n"
      ],
      "metadata": {
        "id": "k9Y3dQm6DU-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to summarize using BART\n",
        "def bart_summarize(text):\n",
        "    # Check if text is empty or None\n",
        "    if not text or text.strip() == \"\":\n",
        "        return \"\"  # Return empty summary for empty text\n",
        "\n",
        "    inputs = tokenizer.batch_encode_plus(\n",
        "        [text],\n",
        "        return_tensors='pt',\n",
        "        truncation=True,  # Truncate if sequence is too long\n",
        "        max_length=1024, # Set a maximum sequence length\n",
        "        padding=\"max_length\" # Pad shorter sequences to the maximum length\n",
        "    ).to(device)\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=4,\n",
        "        max_length=150,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Apply the BART summarization to the reduced dataset\n",
        "df['bart_summary'] = df['cleaned'].progress_apply(bart_summarize)\n"
      ],
      "metadata": {
        "id": "10fhOcfxDZ8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['bart_summary']"
      ],
      "metadata": {
        "id": "s5jPCEyuDbov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize with custom num_beams and max_length\n",
        "def bart_summarize_with_params(text, num_beams=4, max_length=150):\n",
        "    inputs = tokenizer.batch_encode_plus([text], return_tensors='pt', truncation=True).to(device)\n",
        "    summary_ids = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Example: Apply to a single document with different parameters\n",
        "text = df.cleaned.iloc[10]  # Example cleaned document\n",
        "\n",
        "# Experiment with different values of num_beams and max_length\n",
        "results = {\n",
        "    \"num_beams=2, max_length=100\": bart_summarize_with_params(text, num_beams=2, max_length=100),\n",
        "    \"num_beams=4, max_length=150\": bart_summarize_with_params(text, num_beams=4, max_length=150),\n",
        "    \"num_beams=4, max_length=200\": bart_summarize_with_params(text, num_beams=4, max_length=200),\n",
        "    \"num_beams=6, max_length=200\": bart_summarize_with_params(text, num_beams=6, max_length=200),\n",
        "}\n",
        "\n",
        "# Print results for analysis\n",
        "for params, summary in results.items():\n",
        "    print(f\"\\nParameters: {params}\\nSummary: {summary}\")\n"
      ],
      "metadata": {
        "id": "iV_dPTBYDeeR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}